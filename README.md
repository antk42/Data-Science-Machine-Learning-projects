<p align="center"> Data Science Machine Learning projects </p align="center">


All projects are done in **Jupiter notebook**, with datasets attached or the source indicated.

__________________________________________________________________________________________________________________________

| **Projects** | **Description** | **Notes** |
| -------------------- | :--------------------- |:---------------------------|
| Automatic Stacking|Develop a process using sklearn.Pipelines to encompass the stages of training and inference, starting from Feature Engineering and concluding with stacking 10 models into a single pipeline. | Numpy, Pandas, Sklearn, LightGBM, XGBoost, CatBoost |
| Bank Clients Default |Analyzing purchases and suggesting products that may interest customers in this age group. | Pandas, Sqlite3, SqlAlchemy, Numpy, Scipy, Sklearn, Matplotlib, Seaborn, Statsmodels, Phik, Optuna, Shap, CatBoost|
|Blending | Blending is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models | Numpy, Pandas, Sklearn, LightGBM, XGBoost, CatBoost |
|CatBoost| How CatBoost Works| Numpy, Pandas, Sklearn, Shap, CatBoost, Matplotlib, Seaborn |
|Churn bank kaggle |We are faced with the task of classification - it is necessary to determine whether the client will leave in the near future or not. Thus, to achieve the goals of this task, I propose to use the algorithms of Logistic Regression, Random Forest and Catboost. | Numpy, Pandas, Sklearn, Matplotlib, Seaborn, CatBoost |
| Credit Risk | Predict default | Pandas, Numpy, Matplotlib, Phik, Sklearn, Optuna, XGBoost, CatBoost |
| Customer Churn Prediction 2020 |Task is to build a customer retention model for a contractual setting and to predict which customers are retained and which will churn based on the experience theyâ€™ve had as customers of a phone provider. | Pandas, Numpy, Matplotlib, Phik, Sklearn, Optuna, XGBoost, CatBoost |
| Feature Engineering |Feature engineering or feature extraction is the process of extracting features from raw data | Numpy, Pandas |
| Feature Selection |Feature selection methods: Filter methods, Wrapper methods, Embedded methods| Numpy, Pandas, Sklearn, Shap, CatBoost, Matplotlib, Seaborn  |
|Give Me Some Credit |The goal of this competition is to build a model that borrowers can use to help make the best financial decisions.| Pandas, Numpy, Matplotlib, Phik, Sklearn, Optuna, XGBoost, CatBoost |
|Heart Attack EDA and Prediction|A heart attack occurs when the flow of blood to the heart is blocked. Lets try to analyze data set and find some insights to predict heart attacks. |Numpy, Pandas, Sklearn, Tqdm, CatBoost, Matplotlib, Seaborn |
|HMEQ_Data |The consumer credit department of a bank wants to automate the decisionmaking process for approval of home equity lines of credit. To do this, they will follow the recommendations of the Equal Credit Opportunity Act to create an empirically derived and statistically sound credit scoring model | Pandas, Numpy, Matplotlib, Phik, Sklearn, Optuna, XGBoost, CatBoost |
|hyper_parameters_with_grid_search | Workflow for using GridSearch with a machine learning model: | Numpy, Pandas, CV2, Seaborn, Matplotlib, Sklearn, XGBoost |
|LightGBM |How LightGBM Works | Numpy, Pandas, Sklearn, LightGBM|
|optuna_for_XGBoost_ hyperparameter_tuning| Workflow how to tune hyperparameter in Optuna | Numpy, Pandas, Sklearn, Optuna, XGBoost, Matplotlib, Seaborn |
|Pretty pandas, tqdm, pandarallel |Several useful Pandas interface settings that are often handy in work |  Pandas|
|UK Used bmw price prediction | Predicting price for used bmw in UK | Pandas, Numpy, Matplotlib, Phik, Sklearn, Optuna, XGBoost, CatBoost |
|XGBoost|How XGBoost Works | Pandas, Numpy, XGBoost |



